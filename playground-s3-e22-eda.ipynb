{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/yaaangzhou/playground-s3-e22-eda-modeling?scriptVersionId=144582663\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"**Created by Yang Zhou**\n\n**[PLAYGROUND S-3,E-22] ðŸ“ŠEDA**\n\n**12 Sep 2023**","metadata":{}},{"cell_type":"markdown","source":"# <center style=\"font-family: consolas; font-size: 32px; font-weight: bold;\">Predict Health Outcomes of Horses</center>\n<p><center style=\"color:#949494; font-family: consolas; font-size: 20px;\">Playground Series - Season 3, Episode 22</center></p>\n\n***\n\n# <center style=\"font-family: consolas; font-size: 32px; font-weight: bold;\">Insights and Tricks</center>\n\n+ The column `hosptial number` should be a categorical variable because it represents the numbers of different hospitals.\n\n\n\n\n+ After performing the chi-square test, `lesion_3 Counts` was removed, which resulted in a CV score improvement of approximately 0.02. The metric is F1 score.\n\n+ In column `pain`, different sub-labels appear in the test data and training data. For example, `moderate` appeared in the test data and not in training data. The way I handle this situation is to OneHot encode after merging the test and training data.\n\n+ As long as `lesion_2 Counts` is not 0, outcome is not `Died`. We can do the data preprocessing with the code:\n``` python\ndf['lesion_2'] = df['lesion_2'].apply(lambda x:1 if x>0 else 0)\n``` \n\n\n+ [bogoconic1](https://www.kaggle.com/competitions/playground-series-s3e22/discussion/438680) noting that with respect to horses, the normal temperature is 37.8. Deviation in both directions (lower or higher) is attributed with a higher chance of falling in the \"died\" or \"euthanised\" class. So we can create a new feature with the codes below:\n``` python\ndf[\"deviation_from_normal_temp\"] = df[\"rectal_temp\"].apply(lambda x: abs(x - 37.8))\n``` \n\n\n\n\n+ **Key point**: \n    + Using the original data and performing mode filling==>This makes the public score rise from 0.79268 to 0.82317.\n    + Ordinal Encoding works better than OneHot Encoding.==>This makes the public score rise from 0.82317 to 0.82926.\n    + Use Label Encoding with caution! It is prepared for target variables, not for input variables. Here is the [source url.](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)\n    + Using the mode to impute the training data does not perform as well as retaining missing values on the public score. For this reason, we're going to use tree models.","metadata":{}},{"cell_type":"markdown","source":"# <center style=\"font-family: consolas; font-size: 32px; font-weight: bold;\">Version Detail</center>\n\n| Version | Description | Best Public Score |\n|---------|-------------|-----------------|\n| Version 7 | Trying Ordinal Encoding | 0.82926 |\n| Version 6 | Add Neural Network | Not improving |\n| Version 5 | Add Origin Dataset | 0.82317 |\n| Version 4 | Trying Differents Features| Not improving |\n| Version 3 | Add Baseline Modeles | Not improving |\n| Version 2 | Add ML models | 0.81097 |\n| Version 1 | Autogluon Baseline | 0.79878 |\n\n","metadata":{}},{"cell_type":"markdown","source":"# 0. Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', None)\n\nimport math\nfrom scipy import stats\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom collections import Counter\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\n# Preprocessing\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler, RobustScaler, PowerTransformer, QuantileTransformer, OrdinalEncoder, LabelEncoder\nfrom sklearn.impute import SimpleImputer\n\n# Model Selection\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split, GridSearchCV, RepeatedStratifiedKFold\n#import autogluon as ag\n\n# Models\nfrom sklearn.ensemble import HistGradientBoostingClassifier, GradientBoostingClassifier, AdaBoostClassifier,RandomForestClassifier,ExtraTreesClassifier,VotingClassifier, StackingClassifier\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nimport optuna\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")  \nelse:\n    device = torch.device(\"cpu\")   \n    \n# Metrics \nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import roc_auc_score, roc_curve, make_scorer, f1_score\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-28T16:00:41.723534Z","iopub.execute_input":"2023-09-28T16:00:41.724245Z","iopub.status.idle":"2023-09-28T16:00:51.850335Z","shell.execute_reply.started":"2023-09-28T16:00:41.72421Z","shell.execute_reply":"2023-09-28T16:00:51.849306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adjusting plot style\n\nrc = {\n    \"axes.facecolor\": \"#F8F8F8\",\n    \"figure.facecolor\": \"#F8F8F8\",\n    \"axes.edgecolor\": \"#000000\",\n    \"grid.color\": \"#EBEBE7\" + \"30\",\n    \"font.family\": \"serif\",\n    \"axes.labelcolor\": \"#000000\",\n    \"xtick.color\": \"#000000\",\n    \"ytick.color\": \"#000000\",\n    \"grid.alpha\": 0.4\n}\n\nsns.set(rc=rc)\npalette = ['#302c36', '#037d97', '#E4591E', '#C09741',\n           '#EC5B6D', '#90A6B1', '#6ca957', '#D8E3E2']\n\nfrom colorama import Style, Fore\nblk = Style.BRIGHT + Fore.BLACK\nmgt = Style.BRIGHT + Fore.MAGENTA\nred = Style.BRIGHT + Fore.RED\nblu = Style.BRIGHT + Fore.BLUE\nres = Style.RESET_ALL","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-09-28T16:00:51.85246Z","iopub.execute_input":"2023-09-28T16:00:51.853624Z","iopub.status.idle":"2023-09-28T16:00:51.862476Z","shell.execute_reply.started":"2023-09-28T16:00:51.853568Z","shell.execute_reply":"2023-09-28T16:00:51.861516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Load Data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/playground-series-s3e22/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s3e22/test.csv')\nsample_submission = pd.read_csv('/kaggle/input/playground-series-s3e22/sample_submission.csv')\norigin = pd.read_csv('/kaggle/input/horse-survival-dataset/horse.csv')\n\ntrain[\"is_generated\"] = 1\ntest[\"is_generated\"] = 1\norigin[\"is_generated\"] = 0\n\n# Drop column id\ntrain.drop('id',axis=1,inplace=True)\ntest.drop('id',axis=1,inplace=True)\n\ntrain_total = pd.concat([train, origin], ignore_index=True)\ntrain_total.drop_duplicates(inplace=True)\n\ntotal = pd.concat([train_total, test], ignore_index=True)\n\n\nprint('The shape of the train data:', train.shape)\nprint('The shape of the test data:', test.shape)\nprint('The shape of the total data:', total.shape)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:00:51.864514Z","iopub.execute_input":"2023-09-28T16:00:51.865198Z","iopub.status.idle":"2023-09-28T16:00:52.02141Z","shell.execute_reply.started":"2023-09-28T16:00:51.865164Z","shell.execute_reply":"2023-09-28T16:00:52.020292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-09-28T16:00:52.02412Z","iopub.execute_input":"2023-09-28T16:00:52.024549Z","iopub.status.idle":"2023-09-28T16:00:52.059225Z","shell.execute_reply.started":"2023-09-28T16:00:52.024515Z","shell.execute_reply":"2023-09-28T16:00:52.058317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. EDA\n","metadata":{}},{"cell_type":"code","source":"num_var = [column for column in train.columns if train[column].nunique() > 10]\n\nbin_var = [column for column in train.columns if train[column].nunique() == 2]\n\ncat_var = ['temp_of_extremities', 'peripheral_pulse', 'mucous_membrane','capillary_refill_time','pain',\n'peristalsis','abdominal_distention','nasogastric_tube','nasogastric_reflux','rectal_exam_feces','abdomen',\n'abdomo_appearance','lesion_2','surgery', 'age', 'surgical_lesion', 'lesion_3', 'cp_data']\n\ntarget = 'outcome'","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:00:52.060628Z","iopub.execute_input":"2023-09-28T16:00:52.061478Z","iopub.status.idle":"2023-09-28T16:00:52.078137Z","shell.execute_reply.started":"2023-09-28T16:00:52.061451Z","shell.execute_reply":"2023-09-28T16:00:52.076876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cat_var = [column for column in train.columns if train[column].nunique() < 10]\n# cat_var.remove('outcome')","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-09-28T16:00:52.080551Z","iopub.execute_input":"2023-09-28T16:00:52.081183Z","iopub.status.idle":"2023-09-28T16:00:52.085691Z","shell.execute_reply.started":"2023-09-28T16:00:52.081149Z","shell.execute_reply":"2023-09-28T16:00:52.084573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe().T\\\n    .style.bar(subset=['mean'], color=px.colors.qualitative.G10[2])\\\n    .background_gradient(subset=['std'], cmap='Blues')\\\n    .background_gradient(subset=['50%'], cmap='BuGn')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-28T16:00:52.087341Z","iopub.execute_input":"2023-09-28T16:00:52.088053Z","iopub.status.idle":"2023-09-28T16:00:52.187963Z","shell.execute_reply.started":"2023-09-28T16:00:52.088021Z","shell.execute_reply":"2023-09-28T16:00:52.186926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`Hospital number` should appear as a categorical variable, I will handle in feature engineering.","metadata":{}},{"cell_type":"code","source":"def summary(df):\n    sum = pd.DataFrame(df.dtypes, columns=['dtypes'])\n    sum['missing#'] = df.isna().sum()\n    sum['missing%'] = (df.isna().sum())/len(df)\n    sum['uniques'] = df.nunique().values\n    sum['count'] = df.count().values\n    #sum['skew'] = df.skew().values\n    return sum\n\nsummary(train_total).style.background_gradient(cmap='Blues')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-28T16:00:52.189406Z","iopub.execute_input":"2023-09-28T16:00:52.190261Z","iopub.status.idle":"2023-09-28T16:00:52.236055Z","shell.execute_reply.started":"2023-09-28T16:00:52.190233Z","shell.execute_reply":"2023-09-28T16:00:52.234996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(train).style.background_gradient(cmap='Blues')","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:00:52.237542Z","iopub.execute_input":"2023-09-28T16:00:52.237987Z","iopub.status.idle":"2023-09-28T16:00:52.279052Z","shell.execute_reply.started":"2023-09-28T16:00:52.237954Z","shell.execute_reply":"2023-09-28T16:00:52.278032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are more missing cases in `rectal_exam_feces` and `abdomen` columns. In the original data, there are more missing values.","metadata":{}},{"cell_type":"markdown","source":"First, i want to look at the distribution of categorical features. Include the target.","metadata":{}},{"cell_type":"code","source":"columns_cat = [column for column in train.columns if train[column].nunique() < 10]\n\ndef plot_count(df,columns,n_cols,hue):\n    '''\n    # Function to genear countplot\n    df: total data\n    columns: category variables\n    n_cols: num of cols\n    '''\n    n_rows = (len(columns) - 1) // n_cols + 1\n    fig, ax = plt.subplots(n_rows, n_cols, figsize=(17, 4 * n_rows))\n    ax = ax.flatten()\n    \n    for i, column in enumerate(columns):\n        sns.countplot(data=df, x=column, ax=ax[i],hue=hue)\n\n        # Titles\n        ax[i].set_title(f'{column} Counts', fontsize=18)\n        ax[i].set_xlabel(None, fontsize=16)\n        ax[i].set_ylabel(None, fontsize=16)\n        ax[i].tick_params(axis='x', rotation=10)\n\n        for p in ax[i].patches:\n            value = int(p.get_height())\n            ax[i].annotate(f'{value:.0f}', (p.get_x() + p.get_width() / 2, p.get_height()),\n                           ha='center', va='bottom', fontsize=9)\n\n    ylim_top = ax[i].get_ylim()[1]\n    ax[i].set_ylim(top=ylim_top * 1.1)\n    for i in range(len(columns), len(ax)):\n        ax[i].axis('off')\n\n    # fig.suptitle(plotname, fontsize=25, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n    \nplot_count(train,columns_cat,3,'outcome')","metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-09-28T16:00:52.283865Z","iopub.execute_input":"2023-09-28T16:00:52.284152Z","iopub.status.idle":"2023-09-28T16:00:58.873475Z","shell.execute_reply.started":"2023-09-28T16:00:52.284128Z","shell.execute_reply":"2023-09-28T16:00:58.872646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Some observations:**\n\n1. `lesion_2 Counts` and `lesion_3 Counts` appear to have similar distributions. When they are not 0, the horse has a high probability of not dying.\n2. When `cp_data_Counts` is Yes, the target variable is `euthanized` with a small probability.","metadata":{}},{"cell_type":"code","source":"columns_cat = [column for column in train.columns if train[column].nunique() < 10 and column != target]\nplot_count(test,columns_cat,3,None)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:00:58.874448Z","iopub.execute_input":"2023-09-28T16:00:58.87496Z","iopub.status.idle":"2023-09-28T16:01:04.829547Z","shell.execute_reply.started":"2023-09-28T16:00:58.874924Z","shell.execute_reply":"2023-09-28T16:01:04.828666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are data imbalances in some features, which are manifested in:\n1. The `age Counts` column contains a large number of adults.\n2. The `peripheral_pulse_Counts` contains little number of `absent` and `increased`.\n3. It Loos like `lesion_2` and `lesion_3` make no sense.\n\nIn addition, some feature labels that appear in the training set do not appear in the test set. To solve this problem, I will merge the two data after.","metadata":{}},{"cell_type":"markdown","source":"Now let me have a look at numerical features.","metadata":{}},{"cell_type":"code","source":"def plot_pair(df_train,num_var,target,plotname):\n    '''\n    Funtion to make a pairplot:\n    df_train: total data\n    num_var: a list of numeric variable\n    target: target variable\n    '''\n    g = sns.pairplot(data=df_train, x_vars=num_var, y_vars=num_var, hue=target, corner=True)\n    g._legend.set_bbox_to_anchor((0.8, 0.7))\n    g._legend.set_title(target)\n    g._legend.loc = 'upper center'\n    g._legend.get_title().set_fontsize(14)\n    for item in g._legend.get_texts():\n        item.set_fontsize(14)\n\n    plt.suptitle(plotname, ha='center', fontweight='bold', fontsize=25, y=0.98)\n    plt.show()\n\nplot_pair(train,num_var,target,plotname = 'Scatter Matrix with Target')","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-09-28T16:01:04.830846Z","iopub.execute_input":"2023-09-28T16:01:04.831356Z","iopub.status.idle":"2023-09-28T16:01:26.079729Z","shell.execute_reply.started":"2023-09-28T16:01:04.831322Z","shell.execute_reply":"2023-09-28T16:01:26.078653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Some obeservation:**\n\n1. `Total protein` appears to related with the target.\n2. There is no obvious linear relationship between numerical features.","metadata":{}},{"cell_type":"code","source":"df = pd.concat([train[num_var].assign(Source = 'Train'), \n                test[num_var].assign(Source = 'Test')], \n               axis=0, ignore_index = True);\n\nfig, axes = plt.subplots(len(num_var), 3 ,figsize = (16, len(num_var) * 4.2), \n                         gridspec_kw = {'hspace': 0.35, 'wspace': 0.3, 'width_ratios': [0.80, 0.20, 0.20]});\n\nfor i,col in enumerate(num_var):\n    ax = axes[i,0];\n    sns.kdeplot(data = df[[col, 'Source']], x = col, hue = 'Source', ax = ax, linewidth = 2.1)\n    ax.set_title(f\"\\n{col}\",fontsize = 9, fontweight= 'bold');\n    ax.grid(visible=True, which = 'both', linestyle = '--', color='lightgrey', linewidth = 0.75);\n    ax.set(xlabel = '', ylabel = '');\n    ax = axes[i,1];\n    sns.boxplot(data = df.loc[df.Source == 'Train', [col]], y = col, width = 0.25,saturation = 0.90, linewidth = 0.90, fliersize= 2.25, color = '#037d97',\n                ax = ax);\n    ax.set(xlabel = '', ylabel = '');\n    ax.set_title(f\"Train\",fontsize = 9, fontweight= 'bold');\n\n    ax = axes[i,2];\n    sns.boxplot(data = df.loc[df.Source == 'Test', [col]], y = col, width = 0.25, fliersize= 2.25,\n                saturation = 0.6, linewidth = 0.90, color = '#E4591E',\n                ax = ax); \n    ax.set(xlabel = '', ylabel = '');\n    ax.set_title(f\"Test\",fontsize = 9, fontweight= 'bold');\n\nplt.tight_layout();\nplt.show();\n","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-09-28T16:01:26.081192Z","iopub.execute_input":"2023-09-28T16:01:26.081759Z","iopub.status.idle":"2023-09-28T16:01:31.816139Z","shell.execute_reply.started":"2023-09-28T16:01:26.081725Z","shell.execute_reply":"2023-09-28T16:01:31.815077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now, let's look at the distribution of numerical features in the training set.**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(14, len(num_var) * 2.5))\n\nfor idx, column in enumerate(num_var):\n    plt.subplot(len(num_var), 2, idx*2+1)\n    sns.histplot(x=column, hue=\"outcome\", data=train, bins=30, kde=True)\n    plt.title(f\"{column} Distribution for outcome\")\n    plt.ylim(0, train[column].value_counts().max() + 10)\n    \nplt.tight_layout()\nplt.show()","metadata":{"_kg_hide-input":false,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-09-28T16:01:31.817561Z","iopub.execute_input":"2023-09-28T16:01:31.818099Z","iopub.status.idle":"2023-09-28T16:01:36.816051Z","shell.execute_reply.started":"2023-09-28T16:01:31.818064Z","shell.execute_reply":"2023-09-28T16:01:36.815021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_matrix = train[num_var].corr()\nmask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n\nplt.figure(figsize=(15, 12))\nsns.heatmap(corr_matrix, mask=mask, annot=False, cmap='Blues', fmt='.2f', linewidths=1, square=True, annot_kws={\"size\": 9} )\nplt.title('Correlation Matrix', fontsize=15)\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-09-28T16:01:36.81755Z","iopub.execute_input":"2023-09-28T16:01:36.818004Z","iopub.status.idle":"2023-09-28T16:01:37.733762Z","shell.execute_reply.started":"2023-09-28T16:01:36.817969Z","shell.execute_reply":"2023-09-28T16:01:37.732835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Preprocessing and Features Selections\n\n+ During feature selection, I can perform the following tests:\n    + For categorical variables, a chi-square test will be performed to observe their relationship with the target.\n    + We can also use SFS and RFECV for automatic feature selection.\n\nYou can find a complete and detailed tutorial in this [notebook](https://www.kaggle.com/code/alvinleenh/ps3e21-6-basic-feature-selection-techniques), written by [DR. ALVINLEENH](https://www.kaggle.com/alvinleenh).","metadata":{}},{"cell_type":"markdown","source":"## Some test\n\nBefore doing data preprocessing, I would like to perform a chi-square test.","metadata":{}},{"cell_type":"code","source":"def chi_squared_test(df, input_var, target_var, significance_level=0.05):\n    contingency_table = pd.crosstab(df[input_var], df[target_var])\n    chi2, p, _, _ = stats.chi2_contingency(contingency_table)\n    \n    if p < significance_level:\n        print(f'\\033[32m{input_var} has a significant relationship with the target variable.\\033[0m') \n    else:\n        print(f'\\033[31m{input_var} does not have a significant relationship with the target variable.\\033[0m')  \n\nfor i in cat_var:\n    chi_squared_test(train, i, target)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:01:37.735031Z","iopub.execute_input":"2023-09-28T16:01:37.736872Z","iopub.status.idle":"2023-09-28T16:01:37.870548Z","shell.execute_reply.started":"2023-09-28T16:01:37.736838Z","shell.execute_reply":"2023-09-28T16:01:37.86953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this case,`lesion_3` will be dropped.","metadata":{}},{"cell_type":"code","source":"# I'm trying to keep lesion_3 in version 48, let's see if it works better.\n\n# train.drop('lesion_3',axis=1,inplace=True)\n# total.drop('lesion_3',axis=1,inplace=True)\n# cat_var.remove('lesion_3')","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:01:37.872043Z","iopub.execute_input":"2023-09-28T16:01:37.872354Z","iopub.status.idle":"2023-09-28T16:01:37.87915Z","shell.execute_reply.started":"2023-09-28T16:01:37.872324Z","shell.execute_reply":"2023-09-28T16:01:37.878163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"# Mapping target to numbers\n\ntotal[target] = total[target].map({'died':0,'euthanized':1,'lived':2})","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:01:37.880377Z","iopub.execute_input":"2023-09-28T16:01:37.881345Z","iopub.status.idle":"2023-09-28T16:01:37.889639Z","shell.execute_reply.started":"2023-09-28T16:01:37.88131Z","shell.execute_reply":"2023-09-28T16:01:37.888654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I adopted [@MATT OP](https://www.kaggle.com/code/mattop/ps-s3-e22-xgboost-preprocessing)'s preprocessing function and added new features based on it. Let's see how it works.\n\nUnlike me doing one-hot encoding of all categorical variables, he did Ordinal encoding based on the meaning of the features.\n\nAnother benefit of this approach is that the number of features is greatly reduced.","metadata":{}},{"cell_type":"code","source":"# Data preprocessing, code sourse: https://www.kaggle.com/code/mattop/ps-s3-e22-xgboost-preprocessing\ndef preprocessing(df, le_cols, ohe_cols):\n    \n    # Label Encoding for binary cols\n    le = LabelEncoder()    \n    for col in le_cols:\n        df[col] = le.fit_transform(df[col])\n    \n    # OneHot Encoding for category cols\n    df = pd.get_dummies(df, columns = ohe_cols)\n    \n    df[\"pain\"] = df[\"pain\"].replace('slight', 'moderate')\n    df[\"peristalsis\"] = df[\"peristalsis\"].replace('distend_small', 'normal')\n    df[\"rectal_exam_feces\"] = df[\"rectal_exam_feces\"].replace('serosanguious', 'absent')\n    df[\"nasogastric_reflux\"] = df[\"nasogastric_reflux\"].replace('slight', 'none')\n        \n    df[\"temp_of_extremities\"] = df[\"temp_of_extremities\"].fillna(\"normal\").map({'cold': 0, 'cool': 1, 'normal': 2, 'warm': 3})\n    df[\"peripheral_pulse\"] = df[\"peripheral_pulse\"].fillna(\"normal\").map({'absent': 0, 'reduced': 1, 'normal': 2, 'increased': 3})\n    df[\"capillary_refill_time\"] = df[\"capillary_refill_time\"].fillna(\"3\").map({'less_3_sec': 0, '3': 1, 'more_3_sec': 2})\n    df[\"pain\"] = df[\"pain\"].fillna(\"depressed\").map({'alert': 0, 'depressed': 1, 'moderate': 2, 'mild_pain': 3, 'severe_pain': 4, 'extreme_pain': 5})\n    df[\"peristalsis\"] = df[\"peristalsis\"].fillna(\"hypomotile\").map({'hypermotile': 0, 'normal': 1, 'hypomotile': 2, 'absent': 3})\n    df[\"abdominal_distention\"] = df[\"abdominal_distention\"].fillna(\"none\").map({'none': 0, 'slight': 1, 'moderate': 2, 'severe': 3})\n    df[\"nasogastric_tube\"] = df[\"nasogastric_tube\"].fillna(\"none\").map({'none': 0, 'slight': 1, 'significant': 2})\n    df[\"nasogastric_reflux\"] = df[\"nasogastric_reflux\"].fillna(\"none\").map({'less_1_liter': 0, 'none': 1, 'more_1_liter': 2})\n    df[\"rectal_exam_feces\"] = df[\"rectal_exam_feces\"].fillna(\"absent\").map({'absent': 0, 'decreased': 1, 'normal': 2, 'increased': 3})\n    df[\"abdomen\"] = df[\"abdomen\"].fillna(\"distend_small\").map({'normal': 0, 'other': 1, 'firm': 2,'distend_small': 3, 'distend_large': 4})\n    df[\"abdomo_appearance\"] = df[\"abdomo_appearance\"].fillna(\"serosanguious\").map({'clear': 0, 'cloudy': 1, 'serosanguious': 2})\n    \n    # Imputer \n    cols_with_nan = df.drop(target,axis=1).columns[df.drop(target,axis=1).isna().any()].tolist()\n    for feature in cols_with_nan:\n        df[feature].fillna(df[feature].mode()[0], inplace=True)\n     \n    return df  ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-28T16:01:37.891076Z","iopub.execute_input":"2023-09-28T16:01:37.891525Z","iopub.status.idle":"2023-09-28T16:01:37.908184Z","shell.execute_reply.started":"2023-09-28T16:01:37.891491Z","shell.execute_reply":"2023-09-28T16:01:37.907174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total = preprocessing(total, le_cols = [\"surgery\", \"age\", \"surgical_lesion\", \"cp_data\"], ohe_cols = [\"mucous_membrane\"])","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:01:37.909338Z","iopub.execute_input":"2023-09-28T16:01:37.909834Z","iopub.status.idle":"2023-09-28T16:01:37.956701Z","shell.execute_reply.started":"2023-09-28T16:01:37.909804Z","shell.execute_reply":"2023-09-28T16:01:37.955827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I stopped using the feature engineering function after version 50 because in my tests it caused a 0.1 drop in public scores.","metadata":{}},{"cell_type":"code","source":"df_train = total[total[target].notna()]\ndf_test = total[total[target].isna()]\ndf_test.drop(target,axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:01:37.959925Z","iopub.execute_input":"2023-09-28T16:01:37.960175Z","iopub.status.idle":"2023-09-28T16:01:37.972587Z","shell.execute_reply.started":"2023-09-28T16:01:37.960153Z","shell.execute_reply":"2023-09-28T16:01:37.971661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_features = df_test.columns.tolist()\nbin_features = df_test.select_dtypes('bool').columns\n\ndf_train[bin_features] = df_train[bin_features].astype('int64')\ndf_test[bin_features] = df_test[bin_features].astype('int64')","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:01:37.974171Z","iopub.execute_input":"2023-09-28T16:01:37.974766Z","iopub.status.idle":"2023-09-28T16:01:37.987494Z","shell.execute_reply.started":"2023-09-28T16:01:37.97473Z","shell.execute_reply":"2023-09-28T16:01:37.986474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:01:37.990738Z","iopub.execute_input":"2023-09-28T16:01:37.991347Z","iopub.status.idle":"2023-09-28T16:01:38.020141Z","shell.execute_reply.started":"2023-09-28T16:01:37.991323Z","shell.execute_reply":"2023-09-28T16:01:38.019121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Metrics","metadata":{}},{"cell_type":"code","source":"# Metrics\ndef caculate_f1(y_true, y_pred):\n    return f1_score(y_true, y_pred, average = 'micro')","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:01:38.021466Z","iopub.execute_input":"2023-09-28T16:01:38.021857Z","iopub.status.idle":"2023-09-28T16:01:38.026858Z","shell.execute_reply.started":"2023-09-28T16:01:38.021827Z","shell.execute_reply":"2023-09-28T16:01:38.02574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Baseline Model\n\nI will build a baseline model to evaluate the results of feature selection.","metadata":{}},{"cell_type":"code","source":"lgbm_baseline = LGBMClassifier(n_estimators=1000,\n                     max_depth=10,\n                     random_state=42)\n\nf1_results = pd.DataFrame(columns=['Selected_Features', 'F1'])\n\ndef evaluation(df, select_features, note):\n    global f1_results\n    \n    X = df[select_features]\n    Y = df[target]\n    \n    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n    f1_scores = []\n    \n    for train_idx, test_idx in kf.split(X):\n        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_test = Y.iloc[train_idx], Y.iloc[test_idx]\n        \n        lgbm_baseline.fit(X_train, y_train)\n        y_hat = lgbm_baseline.predict(X_test) \n        f1 = caculate_f1(y_test, y_hat)\n        f1_scores.append(f1)\n    \n    average_f1 = np.mean(f1_scores)\n    new_row = {'Selected_Features': note, 'F1': average_f1}\n    f1_results = pd.concat([f1_results, pd.DataFrame([new_row])], ignore_index=True)\n\n    # f1_results = f1_results.append({'Selected_Features': note, 'F1': average_f1}, ignore_index=True)\n    print('====================================')\n    print(note)\n    print(\"Average F1:\", average_f1)\n    print('====================================')\n    return average_f1","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-28T16:01:38.028596Z","iopub.execute_input":"2023-09-28T16:01:38.02901Z","iopub.status.idle":"2023-09-28T16:01:38.042715Z","shell.execute_reply.started":"2023-09-28T16:01:38.028977Z","shell.execute_reply":"2023-09-28T16:01:38.041683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluation(df=df_train,select_features=full_features,note='Baseline')","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:01:38.044325Z","iopub.execute_input":"2023-09-28T16:01:38.044961Z","iopub.status.idle":"2023-09-28T16:01:44.134244Z","shell.execute_reply.started":"2023-09-28T16:01:38.044928Z","shell.execute_reply":"2023-09-28T16:01:44.133327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Selection\n\nThere are many ways to perform feature selection.","metadata":{}},{"cell_type":"markdown","source":"### Correlation\n\nThis function is designed to remove multicollinearity.","metadata":{}},{"cell_type":"code","source":"def correlation(dataset, threshold):\n    col_corr = set()  \n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) >= threshold: \n                colname = corr_matrix.columns[i]                  \n                col_corr.add(colname)\n    return col_corr      \n\ncorr_features = correlation(df_train, 0.35)\ncorr_features","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:01:44.135874Z","iopub.execute_input":"2023-09-28T16:01:44.136567Z","iopub.status.idle":"2023-09-28T16:01:44.172039Z","shell.execute_reply.started":"2023-09-28T16:01:44.136533Z","shell.execute_reply":"2023-09-28T16:01:44.171049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_features = df_test.drop(['abdominal_distention',\n 'abdomo_protein',\n 'capillary_refill_time',\n 'cp_data',\n 'lesion_3',\n 'mucous_membrane_dark_cyanotic',\n 'mucous_membrane_normal_pink',\n 'packed_cell_volume',\n 'peripheral_pulse',\n 'peristalsis',\n 'rectal_exam_feces',\n 'respiratory_rate',\n 'surgical_lesion',\n 'temp_of_extremities',\n 'total_protein'],axis=1).columns.tolist()","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:01:44.179252Z","iopub.execute_input":"2023-09-28T16:01:44.179519Z","iopub.status.idle":"2023-09-28T16:01:44.186297Z","shell.execute_reply.started":"2023-09-28T16:01:44.179496Z","shell.execute_reply":"2023-09-28T16:01:44.184979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluation(df=df_train,select_features=corr_features,note='Corr Features')","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:01:44.187822Z","iopub.execute_input":"2023-09-28T16:01:44.188853Z","iopub.status.idle":"2023-09-28T16:01:49.465062Z","shell.execute_reply.started":"2023-09-28T16:01:44.188818Z","shell.execute_reply":"2023-09-28T16:01:49.464086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Features Importance","metadata":{}},{"cell_type":"code","source":"def f_importance_plot(f_imp):\n    fig = plt.figure(figsize=(12, 0.20*len(f_imp)))\n    plt.title(f'Feature importances', size=16, y=1.05, \n              fontweight='bold')\n    a = sns.barplot(data=f_imp, x='imp', y='feature', linestyle=\"-\", \n                    linewidth=0.5, edgecolor=\"black\",palette='GnBu')\n    plt.xlabel('')\n    plt.xticks([])\n    plt.ylabel('')\n    plt.yticks(size=11)\n    \n    for j in ['right', 'top', 'bottom']:\n        a.spines[j].set_visible(False)\n    for j in ['left']:\n        a.spines[j].set_linewidth(0.5)\n    plt.tight_layout()\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-28T16:01:49.466584Z","iopub.execute_input":"2023-09-28T16:01:49.467245Z","iopub.status.idle":"2023-09-28T16:01:49.475683Z","shell.execute_reply.started":"2023-09-28T16:01:49.467209Z","shell.execute_reply":"2023-09-28T16:01:49.474364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = LGBMClassifier(n_estimators=1000,\n                     max_depth=10,\n                     random_state=42)\nclf.fit(df_train.drop(target,axis=1), df_train[target])\n\nf_imp_df = pd.DataFrame({'feature': df_train.drop(target,axis=1).columns, 'imp': clf.feature_importances_})\nf_imp_df.sort_values(by='imp',ascending=False,inplace=True)\nf_importance_plot(f_imp_df)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:01:49.477344Z","iopub.execute_input":"2023-09-28T16:01:49.477745Z","iopub.status.idle":"2023-09-28T16:01:52.792479Z","shell.execute_reply.started":"2023-09-28T16:01:49.47771Z","shell.execute_reply":"2023-09-28T16:01:52.791557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n# Cost time: 13min30s\n# best_score = 0\n# best_feature_num = 0\n# for i in range(1,f_imp_df.shape[0]):\n#     feature = f_imp_df.head(i).feature.to_list()\n#     # print(f'Trying top {i} features...')\n#     score = evaluation(df=df_train,select_features=feature,note=f'Top {i} Features')\n#     if score > best_score:\n#         best_score = score\n#         best_feature_num = i","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-09-28T16:01:52.794012Z","iopub.execute_input":"2023-09-28T16:01:52.794638Z","iopub.status.idle":"2023-09-28T16:01:52.798937Z","shell.execute_reply.started":"2023-09-28T16:01:52.794593Z","shell.execute_reply":"2023-09-28T16:01:52.797984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_feature_num = 30\nbest_score = 0.7392406127690802\nprint(f'Best feature number is Top {best_feature_num}, Best score is {best_score}')","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:01:52.800402Z","iopub.execute_input":"2023-09-28T16:01:52.80104Z","iopub.status.idle":"2023-09-28T16:01:52.813521Z","shell.execute_reply.started":"2023-09-28T16:01:52.801008Z","shell.execute_reply":"2023-09-28T16:01:52.812539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From this result, it appears that the top 30 features perform best in cross-validation.","metadata":{}},{"cell_type":"code","source":"best_features = f_imp_df.head(best_feature_num).feature.to_list()","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:01:52.815Z","iopub.execute_input":"2023-09-28T16:01:52.81569Z","iopub.status.idle":"2023-09-28T16:01:52.824707Z","shell.execute_reply.started":"2023-09-28T16:01:52.815658Z","shell.execute_reply":"2023-09-28T16:01:52.823842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Modeling\n\nIn version 54, I removed the use of mode imputing, because the public scores after filling were not as high as the scores that retained the missing values. Therefore, I only kept the tree model that allowed for missing values.","metadata":{}},{"cell_type":"code","source":"xgb_cv_scores, xgb_preds = list(), list()\nlgbm_cv_scores, lgbm_preds = list(), list()\ncat_cv_scores, cat_preds = list(), list()\nhist_cv_scores, hist_preds = list(), list()\n\n\nsk = RepeatedStratifiedKFold(n_splits = 5, n_repeats = 1, random_state = 42)\n\nX = df_train[best_features]\nY = df_train[target]\n\ndf_pred = df_test[best_features]\n\nfor i, (train_ix, test_ix) in enumerate(sk.split(X,Y)):\n    X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n    Y_train, Y_test = Y.iloc[train_ix], Y.iloc[test_ix]\n    \n    print('---------------------------------------------------------------')\n    \n    ## XGBClassifer\n    xgb_md = XGBClassifier(n_estimators=1000, \n                           max_depth=3, \n                           early_stopping_rounds=50,\n                           learning_rate=0.55,\n                           min_child_weight=2,\n                           colsample_bytree=0.9,\n                           objective='multi:softmax',\n                           eval_metric='merror',\n                           random_state=1).fit(X_train, Y_train, eval_set=[(X_test, Y_test)], verbose=10000)\n    xgb_pred = xgb_md.predict(X_test)   \n    xgb_f1 = caculate_f1(Y_test, xgb_pred)  \n    print('Fold', i+1, '==> XGBoost oof F1 score is ==>', xgb_f1)\n    xgb_cv_scores.append(xgb_f1)\n    \n    ## Pred\n    xgb_pred_test = xgb_md.predict_proba(df_pred)\n    xgb_preds.append(xgb_pred_test)\n    \n    ## LightGBM\n    lgbm_md = LGBMClassifier(n_estimators=100, random_state=42).fit(X_train, Y_train)\n    lgbm_pred = lgbm_md.predict(X_test)   \n    lgbm_f1 = caculate_f1(Y_test, lgbm_pred)  \n    print('Fold', i+1, '==> LightGBM oof F1 score is ==>', lgbm_f1)\n    lgbm_cv_scores.append(lgbm_f1)\n    \n    ## Pred\n    lgbm_pred_test = lgbm_md.predict_proba(df_pred)\n    lgbm_preds.append(lgbm_pred_test)\n    \n    \n    ## CatBoost\n    cat_md = CatBoostClassifier(loss_function = 'MultiClass',\n                                iterations = 500,\n                                learning_rate = 0.01,\n                                depth = 7,\n                                random_strength = 0.5,\n                                bagging_temperature = 0.7,\n                                border_count = 30,\n                                l2_leaf_reg = 5,\n                                verbose = False, \n                                task_type = 'CPU').fit(X_train, Y_train)\n    cat_pred = cat_md.predict(X_test)   \n    cat_f1 = caculate_f1(Y_test, cat_pred)  \n    print('Fold', i+1, '==> CatBoost oof F1 score is ==>', cat_f1)\n    cat_cv_scores.append(cat_f1)\n    \n    ## Pred\n    cat_pred_test = cat_md.predict_proba(df_pred)\n    cat_preds.append(cat_pred_test)\n    \n    ## HistGradientBoosting\n    hist_md = HistGradientBoostingClassifier(\n                            max_depth=4,          \n                            max_iter=80,         \n                            learning_rate=0.1,     \n                            random_state=42,   \n                            scoring='f1_micro',          \n                            max_leaf_nodes = 21,\n                            l2_regularization = 0.1).fit(X_train, Y_train)\n    hist_pred = hist_md.predict(X_test)   \n    hist_f1 = caculate_f1(Y_test, hist_pred)  \n    print('Fold', i+1, '==> Hist Gradient Boosting oof F1 score is ==>', hist_f1)\n    hist_cv_scores.append(hist_f1)\n    \n    ## Pred\n    hist_pred_test = hist_md.predict_proba(df_pred)\n    hist_preds.append(hist_pred_test)    \n    \n    \n\nprint('---------------------------------------------------------------')\nprint('Average Accuracy of XGBoost model is:', np.mean(xgb_cv_scores))\nprint('Average Accuracy of LGBM model is:', np.mean(lgbm_cv_scores))\nprint('Average Accuracy of Catboost model is:', np.mean(cat_cv_scores))\nprint('Average Accuracy of Hist Gradient Boosting model is:', np.mean(hist_cv_scores))","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:01:52.826265Z","iopub.execute_input":"2023-09-28T16:01:52.826646Z","iopub.status.idle":"2023-09-28T16:02:10.117407Z","shell.execute_reply.started":"2023-09-28T16:01:52.826563Z","shell.execute_reply":"2023-09-28T16:02:10.116653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_submission(prob_list,model_name):\n    '''\n    This function will obtain the probability of each prediction result of the model and generate the final prediction.\n    '''\n    average_probabilities = np.mean(prob_list, axis=0)\n    final_predictions = []\n    for proba in average_probabilities:\n        predicted_label = np.argmax(proba)\n        final_predictions.append(predicted_label)        \n        \n    submission = pd.DataFrame({'id': sample_submission['id'], 'outcome': final_predictions})\n    submission['outcome'] = submission['outcome'].map({0:'died',1:'euthanized',2:'lived'})\n    submission.to_csv(f'{model_name}_submission.csv',index=False)\n    print(f'Result:{model_name}_submission is saved!')\n\n    return submission","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:02:10.12077Z","iopub.execute_input":"2023-09-28T16:02:10.122788Z","iopub.status.idle":"2023-09-28T16:02:10.132536Z","shell.execute_reply.started":"2023-09-28T16:02:10.122752Z","shell.execute_reply":"2023-09-28T16:02:10.131567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_lgbm = get_submission(lgbm_preds,'lgbm')\nsub_xgb = get_submission(xgb_preds,'xgb')\nsub_cat = get_submission(cat_preds,'catboost')\nsub_hist = get_submission(hist_preds,'histgradientboost')","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:02:10.137419Z","iopub.execute_input":"2023-09-28T16:02:10.140342Z","iopub.status.idle":"2023-09-28T16:02:10.253309Z","shell.execute_reply.started":"2023-09-28T16:02:10.140308Z","shell.execute_reply":"2023-09-28T16:02:10.252409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can take a look at the F1 result to find which model is better.","metadata":{}},{"cell_type":"code","source":"f1_results.sort_values('F1',ascending=False)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-09-28T16:02:10.258723Z","iopub.execute_input":"2023-09-28T16:02:10.26143Z","iopub.status.idle":"2023-09-28T16:02:10.288334Z","shell.execute_reply.started":"2023-09-28T16:02:10.261357Z","shell.execute_reply":"2023-09-28T16:02:10.287389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Final Prediction","metadata":{}},{"cell_type":"code","source":"# Get final prediction\npreds = [sub_lgbm,sub_xgb,sub_cat,sub_hist]\nmerged_df = pd.concat(preds)\nfinal_predictions = merged_df.groupby('id')['outcome'].apply(lambda x: x.mode().iloc[0]).reset_index()\nfinal_predictions.to_csv('final_preds.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:02:10.292744Z","iopub.execute_input":"2023-09-28T16:02:10.295017Z","iopub.status.idle":"2023-09-28T16:02:10.536044Z","shell.execute_reply.started":"2023-09-28T16:02:10.294983Z","shell.execute_reply":"2023-09-28T16:02:10.534943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Hyperparameter optimization of individual models\n\nIn this part, I will try to find the best hyperparameters for each base model.","metadata":{}},{"cell_type":"markdown","source":"## Optuna","metadata":{}},{"cell_type":"code","source":"# def objective(trial, data=X, target=Y):\n#     x_train, x_val, y_train, y_val = train_test_split(data, target, test_size=0.18,random_state=3317)\n    \n#     param = {\n#     'eval_metric': ['merror', 'mlogloss'], \n#     'n_estimators': 1000,\n#     'objective':'multi:softmax',\n#     'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n#     'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),\n#     'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),\n#     'gamma': trial.suggest_int('gamma', 1, 20),\n#     'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n#     'colsample_bynode': trial.suggest_categorical('colsample_bynode', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n#     'colsample_bylevel': trial.suggest_categorical('colsample_bylevel', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n#     'subsample': trial.suggest_discrete_uniform('subsample', 0.5, 1.0, 0.05),\n#     'learning_rate': trial.suggest_categorical('learning_rate', [0.002, 0.004, 0.006,0.008,0.01,0.014,0.017,0.02]),\n#     'max_depth': trial.suggest_int('max_depth',10,60 ),\n#     'max_leaves' : trial.suggest_int('max_leaves', 1, 1000)\n#     #'min_child_samples': trial.suggest_int('min_child_samples', 1, 300)\n \n# }\n\n#     model = XGBClassifier(tree_method='gpu_hist', \n#                              gpu_id=0, \n#                              predictor=\"gpu_predictor\",**param)  \n    \n#     model.fit(x_train,y_train,eval_set=[(x_val, y_val)],verbose=0)   \n#     preds = model.predict(x_val)   \n#     score = accuracy_score(y_val, preds)\n    \n#     return score\n\n# study = optuna.create_study(direction='maximize')\n# study.optimize(objective, n_trials=1000)\n# print('Number of finished trials:', len(study.trials))\n# print('Best trial:', study.best_trial.params)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:02:10.541185Z","iopub.execute_input":"2023-09-28T16:02:10.543801Z","iopub.status.idle":"2023-09-28T16:02:10.552678Z","shell.execute_reply.started":"2023-09-28T16:02:10.543762Z","shell.execute_reply":"2023-09-28T16:02:10.551678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LGBM","metadata":{}},{"cell_type":"code","source":"params_lgb = {\n    'objective': 'multiclass', \n    'num_class': 3,\n    'boosting_type' : 'gbdt',\n    'num_leaves': 24,\n    'max_depth': 10,\n    'n_estimators': 450,\n    'learning_rate': 0.08,\n    'random_state': 42,\n    'verbose': -1,\n    'subsample':0.8,\n    'colsample_bytree':0.65,\n    'reg_alpha':0.0001,\n    'reg_lambda':3.5,\n    }","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:02:10.558128Z","iopub.execute_input":"2023-09-28T16:02:10.561341Z","iopub.status.idle":"2023-09-28T16:02:10.570339Z","shell.execute_reply.started":"2023-09-28T16:02:10.561302Z","shell.execute_reply":"2023-09-28T16:02:10.569402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# select_features = ['surgery', 'hospital_number', 'rectal_temp', 'pulse', 'respiratory_rate', 'temp_of_extremities', 'mucous_membrane', 'pain', 'peristalsis', 'abdominal_distention', 'nasogastric_tube', 'nasogastric_reflux', 'nasogastric_reflux_ph', 'rectal_exam_feces', 'abdomen', 'packed_cell_volume', 'total_protein', 'abdomo_appearance', 'abdomo_protein', 'lesion_1', 'cp_data']\nX_opt = df_train[best_features]\nY_opt = df_train[target]\ndf_pred = df_test[best_features]","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:02:10.576054Z","iopub.execute_input":"2023-09-28T16:02:10.578858Z","iopub.status.idle":"2023-09-28T16:02:10.602992Z","shell.execute_reply.started":"2023-09-28T16:02:10.578819Z","shell.execute_reply":"2023-09-28T16:02:10.601623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_opt = LGBMClassifier(**params_lgb).fit(X_opt,Y_opt)\nlgb_preds = lgb_opt.predict(df_pred)\n\nlgb_opt_submission = pd.DataFrame({'id': sample_submission['id'], 'outcome': lgb_preds})\nlgb_opt_submission['outcome'] = lgb_opt_submission['outcome'].map({0:'died',1:'euthanized',2:'lived'})\nlgb_opt_submission.to_csv('lgb_opt_submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:02:10.608598Z","iopub.execute_input":"2023-09-28T16:02:10.611324Z","iopub.status.idle":"2023-09-28T16:02:12.584409Z","shell.execute_reply.started":"2023-09-28T16:02:10.611286Z","shell.execute_reply":"2023-09-28T16:02:12.583361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBoost","metadata":{}},{"cell_type":"code","source":"params_xgb = {'min_child_weight': 5,\n 'reg_alpha': 0.014425096788083052,\n 'reg_lambda': 0.012345176750382126,\n 'gamma': 1,\n 'colsample_bytree': 0.5,\n 'colsample_bynode': 0.7,\n 'colsample_bylevel': 0.7,\n 'subsample': 0.95,\n 'learning_rate': 0.017,\n 'max_depth': 15,\n 'max_leaves': 366}","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:02:12.58609Z","iopub.execute_input":"2023-09-28T16:02:12.586762Z","iopub.status.idle":"2023-09-28T16:02:12.594882Z","shell.execute_reply.started":"2023-09-28T16:02:12.586727Z","shell.execute_reply":"2023-09-28T16:02:12.593938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_opt = XGBClassifier(**params_xgb).fit(X_opt,Y_opt)\nxgb_preds = xgb_opt.predict(df_pred)\n\nxgb_opt_submission = pd.DataFrame({'id': sample_submission['id'], 'outcome': xgb_preds})\nxgb_opt_submission['outcome'] = xgb_opt_submission['outcome'].map({0:'died',1:'euthanized',2:'lived'})\nxgb_opt_submission.to_csv('xgb_opt_submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:02:12.597408Z","iopub.execute_input":"2023-09-28T16:02:12.598034Z","iopub.status.idle":"2023-09-28T16:02:14.153443Z","shell.execute_reply.started":"2023-09-28T16:02:12.597999Z","shell.execute_reply":"2023-09-28T16:02:14.152666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hist Gradient Boosting","metadata":{}},{"cell_type":"code","source":"hist_opt = HistGradientBoostingClassifier(\n    max_depth=4,          \n    max_iter=80,         \n    learning_rate=0.1,     \n    random_state=42,   \n    scoring='f1_micro',          \n    max_leaf_nodes = 21,\n    l2_regularization = 0.1,\n).fit(X_opt,Y_opt)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:02:14.157215Z","iopub.execute_input":"2023-09-28T16:02:14.15905Z","iopub.status.idle":"2023-09-28T16:02:14.601254Z","shell.execute_reply.started":"2023-09-28T16:02:14.159018Z","shell.execute_reply":"2023-09-28T16:02:14.60049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hist_preds = hist_opt.predict(df_pred)\nhist_opt_submission = pd.DataFrame({'id': sample_submission['id'], 'outcome': hist_preds})\nhist_opt_submission['outcome'] = hist_opt_submission['outcome'].map({0:'died',1:'euthanized',2:'lived'})\nhist_opt_submission.to_csv('hist_opt_submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:02:14.60496Z","iopub.execute_input":"2023-09-28T16:02:14.607072Z","iopub.status.idle":"2023-09-28T16:02:14.638136Z","shell.execute_reply.started":"2023-09-28T16:02:14.607041Z","shell.execute_reply":"2023-09-28T16:02:14.637444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. NN by pytorch","metadata":{}},{"cell_type":"code","source":"train_set = torch.tensor(df_train[best_features].values,dtype=torch.float32).to(device)\ntrain_target = torch.tensor(df_train[target].values,dtype=torch.long).to(device)\n\npred_set = torch.tensor(df_test[best_features].values,dtype=torch.float32).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:02:14.640715Z","iopub.execute_input":"2023-09-28T16:02:14.641242Z","iopub.status.idle":"2023-09-28T16:02:17.730713Z","shell.execute_reply.started":"2023-09-28T16:02:14.641209Z","shell.execute_reply":"2023-09-28T16:02:17.729664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[best_features]","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:02:17.732277Z","iopub.execute_input":"2023-09-28T16:02:17.73266Z","iopub.status.idle":"2023-09-28T16:02:17.773145Z","shell.execute_reply.started":"2023-09-28T16:02:17.732622Z","shell.execute_reply":"2023-09-28T16:02:17.772056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Imput shape\ntrain_set.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:02:17.774692Z","iopub.execute_input":"2023-09-28T16:02:17.775037Z","iopub.status.idle":"2023-09-28T16:02:17.78358Z","shell.execute_reply.started":"2023-09-28T16:02:17.775004Z","shell.execute_reply":"2023-09-28T16:02:17.782679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NN(nn.Module):\n    def __init__(self):\n        super(NN,self).__init__()\n        self.layer_1=nn.Linear(train_set.shape[1],128)\n        #self.layer_2=nn.Linear(256,128)\n        self.layer_2=nn.Linear(128,3)\n        self.dropout=nn.Dropout(p=0.5)\n        self.relu = nn.ReLU()\n\n    def forward(self,x):\n        x = x\n        x = self.layer_1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.layer_2(x)\n        x = nn.Softmax()(x)\n        return x","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2023-09-28T16:02:17.784942Z","iopub.execute_input":"2023-09-28T16:02:17.785897Z","iopub.status.idle":"2023-09-28T16:02:17.793918Z","shell.execute_reply.started":"2023-09-28T16:02:17.78586Z","shell.execute_reply":"2023-09-28T16:02:17.79295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\nmodel = NN().to(device)\noptimizer = optim.AdamW(model.parameters(),lr=1e-3,weight_decay=1e-3)\n\nloss_history = []\n\nfor epoch in range(600):\n    pred = model(train_set)\n    pred = pred.squeeze()\n    \n    loss = criterion(pred, train_target)\n    loss_history.append(loss.item())\n    \n    if epoch % 50 == 0 :\n        print(loss)\n        \n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\ntrain_steps = range(1, len(loss_history) + 1)","metadata":{"_kg_hide-output":true,"_kg_hide-input":false,"execution":{"iopub.status.busy":"2023-09-28T16:02:17.795114Z","iopub.execute_input":"2023-09-28T16:02:17.796075Z","iopub.status.idle":"2023-09-28T16:02:20.481677Z","shell.execute_reply.started":"2023-09-28T16:02:17.796037Z","shell.execute_reply":"2023-09-28T16:02:20.480664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(train_steps, loss_history, label='Training Loss')\nplt.xlabel('Training Steps')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('Training Loss Over Time')\nplt.show()","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2023-09-28T16:02:20.482954Z","iopub.execute_input":"2023-09-28T16:02:20.483304Z","iopub.status.idle":"2023-09-28T16:02:20.850462Z","shell.execute_reply.started":"2023-09-28T16:02:20.483271Z","shell.execute_reply":"2023-09-28T16:02:20.849547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction","metadata":{}},{"cell_type":"code","source":"model = NN().to(device)\ntorch.save(model.state_dict(), 'NN.pth')\nmodel.load_state_dict(torch.load('NN.pth'))\n#model.eval()","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:04:28.412826Z","iopub.execute_input":"2023-09-28T16:04:28.413265Z","iopub.status.idle":"2023-09-28T16:04:28.429917Z","shell.execute_reply.started":"2023-09-28T16:04:28.41323Z","shell.execute_reply":"2023-09-28T16:04:28.428643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = []\npred=model(pred_set)\npreds.append(pred)\nprediction = []\nclass_indices = torch.argmax(pred, dim=1)\nclass_mapping = {0: 'died', 1: 'euthanized', 2: 'lived'}\nprediction = [class_mapping[i.item()] for i in class_indices]","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:04:28.680211Z","iopub.execute_input":"2023-09-28T16:04:28.68056Z","iopub.status.idle":"2023-09-28T16:04:28.699845Z","shell.execute_reply.started":"2023-09-28T16:04:28.680531Z","shell.execute_reply":"2023-09-28T16:04:28.698978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nn_submission = pd.DataFrame({'id': sample_submission['id'], 'outcome': prediction})\nnn_submission.to_csv('nn_submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:02:21.771416Z","iopub.status.idle":"2023-09-28T16:02:21.771882Z","shell.execute_reply.started":"2023-09-28T16:02:21.771648Z","shell.execute_reply":"2023-09-28T16:02:21.771671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nn_submission.outcome.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:02:21.773447Z","iopub.status.idle":"2023-09-28T16:02:21.774093Z","shell.execute_reply.started":"2023-09-28T16:02:21.773851Z","shell.execute_reply":"2023-09-28T16:02:21.773873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. Baseline with Autogluon(Removed after version 26)\n\n~~At the beginning, I'm gonna build a baseline model using an automated machine learning framework.~~","metadata":{}},{"cell_type":"code","source":"#!pip install autogluon","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:02:21.775599Z","iopub.status.idle":"2023-09-28T16:02:21.776495Z","shell.execute_reply.started":"2023-09-28T16:02:21.776224Z","shell.execute_reply":"2023-09-28T16:02:21.776252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from autogluon.tabular import TabularDataset, TabularPredictor\n\n#predictor = TabularPredictor(label='outcome').fit(df_train)\n#preds = predictor.predict(df_test.drop(target,axis=1))\n#preds = preds.map({0:'died',1:'euthanized',2:'lived'})\n#auto_submission = pd.DataFrame({'id': sample_submission['id'], 'outcome': preds})\n#auto_submission.to_csv('auto_submission.csv',index=False)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-09-28T16:02:21.777968Z","iopub.status.idle":"2023-09-28T16:02:21.778432Z","shell.execute_reply.started":"2023-09-28T16:02:21.778187Z","shell.execute_reply":"2023-09-28T16:02:21.778215Z"},"trusted":true},"execution_count":null,"outputs":[]}]}